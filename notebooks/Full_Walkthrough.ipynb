{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine all of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the locations of the files for X_train, X_test, and y_train. Also, there is a file that contains information about the individual stations that can be useful for models that learn for each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain_dir = 'solar/data/kaggle_solar/train/'\n",
    "Xtest_dir = 'solar/data/kaggle_solar/test'\n",
    "ytrain_file = 'solar/data/kaggle_solar/train.csv'\n",
    "station_file = 'solar/data/kaggle_solar/station_info.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the various files. This is mostly done so that any file updates during testing are carried over to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import solar.wrangle.wrangle\n",
    "import solar.wrangle.subset\n",
    "import solar.wrangle.engineer\n",
    "import solar.analyze.model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters that will be used to set up the data. There are some parameters that determine the size and shape of the data but have effects other than setting up feature columns. This includes the dates that are included for testing and training, the stations considered, and whether to have X values correspond to a date or to a specific date/station combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose up to 98 stations; not specifying a station means to use all that fall within the given lats and longs. If the\n",
    "# parameter 'all' is given, then it will use all stations no matter the provided lats and longs\n",
    "#station = ['ACME', 'BEAV']\n",
    "station = ['all']\n",
    "\n",
    "# Determine which dates will be used to train the model. No specified date means use the entire set from 1994-01-01 \n",
    "# until 2007-12-31.\n",
    "train_dates = ['2005-07-29','2005-08-29']\n",
    "\n",
    "# Determine the test X values to produce. There is no practical purpose to use fewer than all of the points other than\n",
    "# for testing. Again, not choosing a date will use 2008-01-01 through 2012-11-30.\n",
    "test_dates = ['2008-12-29', '2009-01-30']\n",
    "#train_dates = []\n",
    "#test_dates = []\n",
    "\n",
    "# The last parameter that is not specifically involved in feature selection in the layout to be used for training\n",
    "# I have switched to almost always training for each individual station rather than having a single row for a date.\n",
    "# However, I am still not beating the benchmark, and the file would grow too large to have the benchmark laid out\n",
    "# with a row for each station, so I'll keep the switch. True means that each station has a row (5113 dates X 98\n",
    "# stations to train the data). False means that there are 5113 rows that are being used to train the data.\n",
    "station_layout = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we start to layout the features to include. The two most important (and complicated) are 'absolute', which just reports out the weather variables at specific GEFS, times, and models, and 'relative' which uses a grid to identify nearby GEFS for weather measurements based on the location of the station. The second option makes the most sense when using the station_layout above, but it will work with either layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A very simple model would just take the average value of all variables at all locations, using all models over the\n",
    "# course of the day. Here, only the var parameter and one value of the model is expanded. \n",
    "# All of the other axes are aggregated using an aggregation function. In this case, the mean value. \n",
    "# This will provide a 15 aggregated columns for model 0 and 15 aggregated columns for the mean of models 1 though 10.\n",
    "# In this case, setting station_layout to false would make the most sense because the measurements will be repeated\n",
    "# for each station. However, for consistency in this walkthough, I will just keep it in the station_layout.\n",
    "\n",
    "# Dimensions without aggregation\n",
    "\n",
    "# Use all variables\n",
    "var = ['all']\n",
    "\n",
    "# Keep model 0 (the default model) as a column for each of the variables (aggregated over other dimensions)\n",
    "model1 = [0]\n",
    "\n",
    "# Dimensions with aggregation\n",
    "\n",
    "# Aggregate over all other models (excluding model 0, which is used directly)\n",
    "model2 = range(1,11)\n",
    "\n",
    "# Aggregate over all times\n",
    "times = ['all']\n",
    "\n",
    "# Aggregate over all latitudes which surround Mesonet stations (exclude those that are outside of the main grid)\n",
    "lats = range(36,41)\n",
    "\n",
    "# Same as for lats\n",
    "longs = range(257,267)\n",
    "\n",
    "all_avgs = {'type':'absolute', 'full_axes':{'var':var, 'models':model1}, \n",
    "            'agg_axes':{'models':[model2,[np.mean]], 'times':[times, [np.mean, np.sum]], 'lats':[lats,[np.median]],\n",
    "                        'longs':[longs,[np.median]]}}\n",
    "\n",
    "avgs_feats = [all_avgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A similar example using a surrounding grid for each station. There are no lat or long options for this type of\n",
    "# feature set\n",
    "\n",
    "# Dimensions without aggregation\n",
    "\n",
    "# Use all variables\n",
    "var = ['all']\n",
    "\n",
    "# Keep model 0 (the default model) as a column for each of the variables (aggregated over other dimensions)\n",
    "model1 = [0]\n",
    "\n",
    "# Dimensions with aggregation\n",
    "\n",
    "# Aggregate over all other models (excluding model 0, which is used directly)\n",
    "model2 = range(1,11)\n",
    "\n",
    "# Aggregate over all times\n",
    "times = ['all']\n",
    "\n",
    "# Create a column for each member of the grid. All or nothing for gefs now. Could specify but currently see no need\n",
    "# for it. We could also take an aggregate measure of the gefs (including interpolate). That doesn't work for the\n",
    "# other dimensions\n",
    "\n",
    "gefs = ['all']\n",
    "\n",
    "grid_avgs = {'type':'relative', 'full_axes':{'var':var, 'models':model1, 'gefs':gefs}, \n",
    "            'agg_axes':{'models':[model2,[np.mean]], 'times':[times, [np.mean, np.sum]]}}\n",
    "\n",
    "grid_feats = [grid_avgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A similar example using a surrounding grid for each station. Now, just average over the grid\n",
    "\n",
    "# Dimensions without aggregation\n",
    "\n",
    "# Use all variables\n",
    "var = ['all']\n",
    "\n",
    "# Keep model 0 (the default model) as a column for each of the variables (aggregated over other dimensions)\n",
    "model1 = [0]\n",
    "\n",
    "# Dimensions with aggregation\n",
    "\n",
    "# Aggregate over all other models (excluding model 0, which is used directly)\n",
    "model2 = range(1,11)\n",
    "\n",
    "# Aggregate over all times\n",
    "times = ['all']\n",
    "\n",
    "# Create a column for each member of the grid. All or nothing for gefs now. Could specify but currently see no need\n",
    "# for it. We could also take an aggregate measure of the gefs (including interpolate). That doesn't work for the\n",
    "# other dimensions\n",
    "\n",
    "gefs = ['all']\n",
    "\n",
    "grid_avgs = {'type':'relative', 'full_axes':{'var':var, 'models':model1}, \n",
    "            'agg_axes':{'models':[model2,[np.mean]], 'times':[times, [np.mean, np.sum]], 'gefs':[gefs, [np.mean]]}}\n",
    "\n",
    "gefs_mean_feats = [grid_avgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This just uses the station_names as another feature\n",
    "stat_names = {'type':'station_names'}\n",
    "stat_feats = [stat_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if I am modifying code for any of these pythons\n",
    "reload(solar.wrangle.wrangle)\n",
    "reload(solar.wrangle.subset)\n",
    "reload(solar.wrangle.engineer)\n",
    "\n",
    "from solar.wrangle.wrangle import SolarData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%prun input_data = SolarData.load(Xtrain_dir, ytrain_file, Xtest_dir, station_file, train_dates, test_dates, station,\n",
    "                                 station_layout, new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
